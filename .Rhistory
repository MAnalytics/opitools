head(doc_words)
wordcloud2(data=doc_words, size = 0.7, shape = 'pentagon')
nrow(doc_words)
wordcloud2(data=doc_words[1:1000,], size = 0.7, shape = 'pentagon')
wordcloud2(data=doc_words[1:1000,], size = 0.7, shape = 'pentagon')
wordcloud2(data=doc_words[1:1000,], size = 0.7, shape = 'pentagon')
wordcloud2(data=doc_words[1:1000,], size = 0.7, shape = 'pentagon')
dev.new()
dev.new()
wordcloud2(data=doc_words[1:1000,], size = 0.7, shape = 'pentagon')
wordcloud2(data=doc_words[1:1000,], size = 0.7, shape = 'pentagon')
wordcloud2(data=doc_words[1:1000,], size = 0.7, shape = 'pentagon')
library(opitools)
library(rhub)
install.packages("rhub")
library(rhub)
validate_email()
check()
check_for_cran()
check_for_cran()
check_for_cran()
check_for_cran()
check_for_cran()
check_on_windows()
library(textdata)
lexicon_afinn(dir = "data/")
afinn <- readRDS("C:/Users/monsu/Documents/GitHub/opitools/data/afinn/afinn_111.rds")
save(afinn, file="afinn.rda")
load(file="/data/afinn.rda")
rm(list=ls())
load(file="data/afinn.rda")
afinn
afinn_111 <- afinn
save(afinn_111, file="afinn_111.rda")
check_on_windows()
check_on_debian()
cran_prep <- check_for_cran()
cran_prep$cran_summary()
cran_prep <- check_for_cran()
check_on_windows()
library(goodpractice) #to check...
path <- "C:/Users/monsu/Documents/GitHub/opitools"
g <- gp(path, checks = all_checks()[3:16], quiet = FALSE)
g
g
library(goodpractice) #to check...
path <- "C:/Users/monsu/Documents/GitHub/opitools"
g <- gp(path, checks = all_checks()[3:16], quiet = FALSE)
g
library(goodpractice) #to check...
path <- "C:/Users/monsu/Documents/GitHub/opitools"
g <- gp(path, checks = all_checks()[3:16], quiet = FALSE)
g
tribble(
~ Legend, ~ x, ~ y,
"LogFreq_vs_LogRank", -1, -1,
"LogFreq_vs_LogRank", 1, 1,
"Idealized_Zipfs_Law", -1, 1,
"Idealized_Zipfs_Law", 1, -1
)
tibble(
~ Legend, ~ x, ~ y,
"LogFreq_vs_LogRank", -1, -1,
"LogFreq_vs_LogRank", 1, 1,
"Idealized_Zipfs_Law", -1, 1,
"Idealized_Zipfs_Law", 1, -1
)
tweets[,1]
tweets_dat <- data.frame(text=tweets[,1])
dim(tweets_dat)
head(tweets_dat)
dat <- tibble(
~ Legend, ~ x, ~ y,
"LogFreq_vs_LogRank", -1, -1,
"LogFreq_vs_LogRank", 1, 1,
"Idealized_Zipfs_Law", -1, 1,
"Idealized_Zipfs_Law", 1, -1
)
library(tibble)
dat <- tibble(
~ Legend, ~ x, ~ y,
"LogFreq_vs_LogRank", -1, -1,
"LogFreq_vs_LogRank", 1, 1,
"Idealized_Zipfs_Law", -1, 1,
"Idealized_Zipfs_Law", 1, -1
)
dat <- tibble(
~Legend, ~x, ~y,
"LogFreq_vs_LogRank", -1, -1,
"LogFreq_vs_LogRank", 1, 1,
"Idealized_Zipfs_Law", -1, 1,
"Idealized_Zipfs_Law", 1, -1
)
dat <- tribble(
~Legend, ~x, ~y,
"LogFreq_vs_LogRank", -1, -1,
"LogFreq_vs_LogRank", 1, 1,
"Idealized_Zipfs_Law", -1, 1,
"Idealized_Zipfs_Law", 1, -1
)
dat
path <- "C:/Users/monsu/Documents/GitHub/akmedoids"
g <- gp(path, checks = all_checks()[3:16], quiet = FALSE)
g
path <- "C:/Users/monsu/Documents/GitHub/opitools"
g <- gp(path, checks = all_checks()[3:16], quiet = FALSE)
g
check_on_windows()
check_on_windows()
tweets_dat <- data.frame(text=tweets[,1])
tweets_dat <- data.frame(text=tweets[,1])
tweets_dat
textdoc = tweets_dat
outp <- list()
dat <- list(textdoc)
series <- tibble()
#tokenize document
tokenized <- tibble(text = as.character(unlist(dat)))%>%
unnest_tokens(word, text)%>% #tokenize
dplyr::select(everything())
series <- tokenized
doc_words <- series %>%
dplyr::count(word, sort = TRUE) %>%
dplyr::ungroup()
total_words <- doc_words %>%
summarise(total = sum(n))
doc_words <- cbind(doc_words, total_words)
library(dplyr)
library(tidytext)
doc_words <- series %>%
dplyr::count(word, sort = TRUE) %>%
dplyr::ungroup()
dat <- list(textdoc)
series <- tibble()
tokenized <- tibble(text = as.character(unlist(dat)))%>%
unnest_tokens(word, text)%>% #tokenize
dplyr::select(everything())
series <- tokenized
doc_words <- series %>%
dplyr::count(word, sort = TRUE) %>%
dplyr::ungroup()
doc_words
total_words <- doc_words %>%
summarise(total = sum(n))
doc_words <- cbind(doc_words, total_words)
doc_words
doc_words %>%
mutate(rank = row_number(),
`term_freq` = n / total)
doc_words %>%
mutate(rank = row_number(),
`term_freq` = n / total)
check_on_windows()
check_on_debian()
library(opitools)
system("R CMD Rd2pdf . --title = Package opitools --output=./opitools_user_manual.pdf --force --no-clean --internals")
use_data(afinn_111, internal = TRUE)
library(usethis)
use_data(afinn_111, internal = TRUE)
new<-readRDS(file="C:/Users/monsu/Documents/GitHub/opitools/data/sysdata.rda")
new<-load(file="C:/Users/monsu/Documents/GitHub/opitools/data/sysdata.rda")
new
use_data(afinn_111, name="afinn_111", internal = TRUE)
afinn_111
use_data(afinn_111, overwrite = TRUE, internal = TRUE)
use_data(afinn_111, overwrite = TRUE, internal = TRUE)
?data
sysdata <- afinn_111
use_data(sysdata, overwrite = TRUE, internal = TRUE)
library(opitools)
rm(list=ls())
policing_otd
covid_keys
check_on_windows()
system("R CMD Rd2pdf . --title = Package opitools --output=./opitools_user_manual.pdf --force --no-clean --internals")
check_on_windows()
check_on_windows()
check_on_debian()
check_on_windows()
tweets_dat <- data.frame(text=tweets[,1])
library(opitools)
tweets_dat <- data.frame(text=tweets[,1])
word_distrib(textdoc = tweets_dat)
check_on_windows()
check_on_debian()
library(opitools)
get_x
tweets_dat <- data.frame(text=tweets[,1])
outp <- list()
dat <- list(textdoc)
series <- tibble()
#tokenize document
tokenized <- tibble(text = as.character(unlist(dat)))%>%
unnest_tokens(word, text)%>% #tokenize
select(everything())
series <- tokenized
#compute term frequencies
doc_words <- series %>%
count(word, sort = TRUE) %>%
ungroup()
total_words <- doc_words %>%
summarise(total = sum(n))
doc_words <- cbind(doc_words, total_words)
#plot log(rank order) vs. log(word frequency)
freq_by_rank <- doc_words %>%
mutate(rank = row_number(),
`term_freq` = n / total)
#---
data <- word <- text <- everything <- summarise <- n <- mutate <-
row_number <- total <- aes <- `term_freq` <- geom_line <-
scale_x_log10 <- scale_y_log10 <- filter <- lm <- geom_abline <- xlab <-
ylab <- theme_light <-Legend <- scale_colour_brewer <- x <- y <-
position_nudge <- ggplot_build <- NULL
# ggplot(freq_by_rank, aes(rank, `term_freq`)) +
#   geom_line() +
#   scale_x_log10() +
#   scale_y_log10()
#compare the distribution to a simple regression line
#examine the head, tail and mid section of the plot
lower_rank <- freq_by_rank %>%
filter(rank < 500)%>%
rename(`term_freq` = 5)
int_slope <- lm(log10(`term_freq`) ~ log10(rank), data = lower_rank)
#colors <- c("Sepal Width" = "blue", "Petal Length" = "red")
#create a fictitious data to borrow its legend
dat <- tribble(
~Legend, ~x, ~y,
"LogFreq_vs_LogRank", -1, -1,
"LogFreq_vs_LogRank", 1, 1,
"Idealized_Zipfs_Law", -1, 1,
"Idealized_Zipfs_Law", 1, -1
)
p <- ggplot(data=dat, aes(x, y, color=Legend)) +
geom_line(size = 1.6, alpha = 0.8) +
scale_color_manual(values = c(LogFreq_vs_LogRank = "red",
Idealized_Zipfs_Law = "gray40"))
#get legend
leg <- get_legend(p)
freq_by_rank <- freq_by_rank %>%
mutate(group=1)
#get index for labels
xx <-round(nrow(freq_by_rank)/10, digits=0) - 2
#create sequence
idx <- xx
for(i in 2:10){#i<-2
idx <- rbind(idx, idx[i-1] + xx)
#flush.console()
#print(i)
}
idx <- as.vector(idx)
#plot to first get x labels
get_x <- ggplot(freq_by_rank, aes(rank, term_freq,
color="Log(freq) vs. Log(r)")) +
geom_line(size = 1.6, alpha = 0.8, colour="red") +
geom_point(colour="red", alpha=0, size=1) +
geom_text(data=freq_by_rank[idx,],
aes(rank, term_freq,label=word))+
labs(title="Checking text document against Zipf's law")+
scale_x_log10() +
scale_y_log10()
get_x <- ggplot_build(get_x)$layout$panel_params[[1]]$x$get_labels()
library(ggplot2)
library(dplyr)
dat <- list(textdoc)
series <- tibble()
#tokenize document
tokenized <- tibble(text = as.character(unlist(dat)))%>%
unnest_tokens(word, text)%>% #tokenize
select(everything())
series <- tokenized
#compute term frequencies
doc_words <- series %>%
count(word, sort = TRUE) %>%
ungroup()
total_words <- doc_words %>%
summarise(total = sum(n))
doc_words <- cbind(doc_words, total_words)
#plot log(rank order) vs. log(word frequency)
freq_by_rank <- doc_words %>%
mutate(rank = row_number(),
`term_freq` = n / total)
#---
data <- word <- text <- everything <- summarise <- n <- mutate <-
row_number <- total <- aes <- `term_freq` <- geom_line <-
scale_x_log10 <- scale_y_log10 <- filter <- lm <- geom_abline <- xlab <-
ylab <- theme_light <-Legend <- scale_colour_brewer <- x <- y <-
position_nudge <- ggplot_build <- NULL
# ggplot(freq_by_rank, aes(rank, `term_freq`)) +
#   geom_line() +
#   scale_x_log10() +
#   scale_y_log10()
#compare the distribution to a simple regression line
#examine the head, tail and mid section of the plot
lower_rank <- freq_by_rank %>%
filter(rank < 500)%>%
rename(`term_freq` = 5)
int_slope <- lm(log10(`term_freq`) ~ log10(rank), data = lower_rank)
#colors <- c("Sepal Width" = "blue", "Petal Length" = "red")
#create a fictitious data to borrow its legend
dat <- tribble(
~Legend, ~x, ~y,
"LogFreq_vs_LogRank", -1, -1,
"LogFreq_vs_LogRank", 1, 1,
"Idealized_Zipfs_Law", -1, 1,
"Idealized_Zipfs_Law", 1, -1
)
p <- ggplot(data=dat, aes(x, y, color=Legend)) +
geom_line(size = 1.6, alpha = 0.8) +
scale_color_manual(values = c(LogFreq_vs_LogRank = "red",
Idealized_Zipfs_Law = "gray40"))
#get legend
leg <- get_legend(p)
freq_by_rank <- freq_by_rank %>%
mutate(group=1)
#get index for labels
xx <-round(nrow(freq_by_rank)/10, digits=0) - 2
#create sequence
idx <- xx
for(i in 2:10){#i<-2
idx <- rbind(idx, idx[i-1] + xx)
#flush.console()
#print(i)
}
idx <- as.vector(idx)
#plot to first get x labels
get_x <- ggplot(freq_by_rank, aes(rank, term_freq,
color="Log(freq) vs. Log(r)")) +
geom_line(size = 1.6, alpha = 0.8, colour="red") +
geom_point(colour="red", alpha=0, size=1) +
geom_text(data=freq_by_rank[idx,],
aes(rank, term_freq,label=word))+
labs(title="Checking text document against Zipf's law")+
scale_x_log10() +
scale_y_log10()
get_x <- ggplot_build(get_x)$layout$panel_params[[1]]$x$get_labels()
library(cowplot)
dat <- list(textdoc)
series <- tibble()
#tokenize document
tokenized <- tibble(text = as.character(unlist(dat)))%>%
unnest_tokens(word, text)%>% #tokenize
select(everything())
series <- tokenized
#compute term frequencies
doc_words <- series %>%
count(word, sort = TRUE) %>%
ungroup()
total_words <- doc_words %>%
summarise(total = sum(n))
doc_words <- cbind(doc_words, total_words)
#plot log(rank order) vs. log(word frequency)
freq_by_rank <- doc_words %>%
mutate(rank = row_number(),
`term_freq` = n / total)
#---
data <- word <- text <- everything <- summarise <- n <- mutate <-
row_number <- total <- aes <- `term_freq` <- geom_line <-
scale_x_log10 <- scale_y_log10 <- filter <- lm <- geom_abline <- xlab <-
ylab <- theme_light <-Legend <- scale_colour_brewer <- x <- y <-
position_nudge <- ggplot_build <- NULL
# ggplot(freq_by_rank, aes(rank, `term_freq`)) +
#   geom_line() +
#   scale_x_log10() +
#   scale_y_log10()
#compare the distribution to a simple regression line
#examine the head, tail and mid section of the plot
lower_rank <- freq_by_rank %>%
filter(rank < 500)%>%
rename(`term_freq` = 5)
int_slope <- lm(log10(`term_freq`) ~ log10(rank), data = lower_rank)
#colors <- c("Sepal Width" = "blue", "Petal Length" = "red")
#create a fictitious data to borrow its legend
dat <- tribble(
~Legend, ~x, ~y,
"LogFreq_vs_LogRank", -1, -1,
"LogFreq_vs_LogRank", 1, 1,
"Idealized_Zipfs_Law", -1, 1,
"Idealized_Zipfs_Law", 1, -1
)
p <- ggplot(data=dat, aes(x, y, color=Legend)) +
geom_line(size = 1.6, alpha = 0.8) +
scale_color_manual(values = c(LogFreq_vs_LogRank = "red",
Idealized_Zipfs_Law = "gray40"))
#get legend
leg <- get_legend(p)
freq_by_rank <- freq_by_rank %>%
mutate(group=1)
#get index for labels
xx <-round(nrow(freq_by_rank)/10, digits=0) - 2
#create sequence
idx <- xx
for(i in 2:10){#i<-2
idx <- rbind(idx, idx[i-1] + xx)
#flush.console()
#print(i)
}
idx <- as.vector(idx)
#plot to first get x labels
get_x <- ggplot(freq_by_rank, aes(rank, term_freq,
color="Log(freq) vs. Log(r)")) +
geom_line(size = 1.6, alpha = 0.8, colour="red") +
geom_point(colour="red", alpha=0, size=1) +
geom_text(data=freq_by_rank[idx,],
aes(rank, term_freq,label=word))+
labs(title="Checking text document against Zipf's law")+
scale_x_log10() +
scale_y_log10()
get_x <- ggplot_build(get_x)$layout$panel_params[[1]]$x$get_labels()
get_x
get_x <- as.numeric(get_x[!is.na(get_x)])
get_x
tweets_dat <- data.frame(text=tweets[,1])
textdoc =
tweets_dat
outp <- list()
#check that data is one-column
if(dim(textdoc)[2]!=1){
stop(paste("Dataframe must include only one column",
"containing the text records!!", sep=" "))
}
dat <- list(textdoc)
series <- tibble()
#tokenize document
tokenized <- tibble(text = as.character(unlist(dat)))%>%
unnest_tokens(word, text)%>% #tokenize
select(everything())
series <- tokenized
#compute term frequencies
doc_words <- series %>%
count(word, sort = TRUE) %>%
ungroup()
total_words <- doc_words %>%
summarise(total = sum(n))
doc_words <- cbind(doc_words, total_words)
#plot log(rank order) vs. log(word frequency)
freq_by_rank <- doc_words %>%
mutate(rank = row_number(),
`term_freq` = n / total)
#---
data <- word <- text <- everything <- summarise <- n <- mutate <-
row_number <- total <- aes <- `term_freq` <- geom_line <-
scale_x_log10 <- scale_y_log10 <- filter <- lm <- geom_abline <- xlab <-
ylab <- theme_light <-Legend <- scale_colour_brewer <- x <- y <-
position_nudge <- ggplot_build <- NULL
# ggplot(freq_by_rank, aes(rank, `term_freq`)) +
#   geom_line() +
#   scale_x_log10() +
#   scale_y_log10()
#compare the distribution to a simple regression line
#examine the head, tail and mid section of the plot
lower_rank <- freq_by_rank %>%
filter(rank < 500)%>%
rename(`term_freq` = 5)
int_slope <- lm(log10(`term_freq`) ~ log10(rank), data = lower_rank)
#colors <- c("Sepal Width" = "blue", "Petal Length" = "red")
#create a fictitious data to borrow its legend
dat <- tribble(
~Legend, ~x, ~y,
"LogFreq_vs_LogRank", -1, -1,
"LogFreq_vs_LogRank", 1, 1,
"Idealized_Zipfs_Law", -1, 1,
"Idealized_Zipfs_Law", 1, -1
)
p <- ggplot(data=dat, aes(x, y, color=Legend)) +
geom_line(size = 1.6, alpha = 0.8) +
scale_color_manual(values = c(LogFreq_vs_LogRank = "red",
Idealized_Zipfs_Law = "gray40"))
#get legend
leg <- get_legend(p)
freq_by_rank <- freq_by_rank %>%
mutate(group=1)
#get index for labels
xx <-round(nrow(freq_by_rank)/10, digits=0) - 2
#create sequence
idx <- xx
for(i in 2:10){#i<-2
idx <- rbind(idx, idx[i-1] + xx)
#flush.console()
#print(i)
}
idx <- as.vector(idx)
#plot to first get x labels
get_x <- ggplot(freq_by_rank, aes(rank, term_freq,
color="Log(freq) vs. Log(r)")) +
geom_line(size = 1.6, alpha = 0.8, colour="red") +
geom_point(colour="red", alpha=0, size=1) +
geom_text(data=freq_by_rank[idx,],
aes(rank, term_freq,label=word))+
labs(title="Checking text document against Zipf's law")+
scale_x_log10() +
scale_y_log10()
get_x <- ggplot_build(get_x)$layout$panel_params[[1]]$x$get_labels()
get_x
get_x <- as.numeric(get_x[!is.na(get_x)])
get_x
#modify the last label
length(get_x)
(length(get_x)-1)
length(get_x)
get_x[(length(get_x)-1)]
get_x[length(get_x)]
get_x[length(get_x)] + get_x[(length(get_x)-1)]
#modify the last label
round((get_x[length(get_x)] + get_x[(length(get_x)-1)])/2, digits=0)
get_x
c(get_x[1:(length(get_x)-1)],round((get_x[length(get_x)] +
get_x[(length(get_x)-1)])/2, digits=0))
tweets_dat <- data.frame(text=tweets[,1])
word_distrib(textdoc = tweets_dat)
library(opitools)
system("R CMD Rd2pdf . --title = Package opitools --output=./finalmanual.pdf --force --no-clean --internals")
check_on_windows()
