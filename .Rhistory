series$groups <- ntile(as.numeric(series$rowname), n_grp)
series <- series %>%
select(-c(rowname))
nc == 2
#tokenize
tokenize_series <- series %>%
group_by(groups)%>%
#paste(text, collapse = " ")
summarise(text2 = paste(text, collapse=" "))%>%
rename(text=text2)%>%
unnest_tokens(word, text) %>%
dplyr::filter(!word %in% words_to_filter)
tokenize_series
#removing stopwords
tokenize_series <- tokenize_series[!tokenize_series$word %in% stopwords("english"),]
tokenize_series
#calculate 'tf' across all groups & 'tf_idf' value per groups
tf <- tokenize_series %>%
select(word)%>%
group_by(word)%>%
summarise(c=n())%>%
#bind_tf_idf(word, groups, c) %>%
arrange(desc(c))
tf
tf_idf <- tokenize_series %>%
group_by(word, groups)%>%
summarise(c=n())%>%
bind_tf_idf(word, groups, c) %>%
arrange(desc(tf_idf))%>%
group_by(groups)%>%
#remove duplicate
filter(!duplicated(word))
tf_idf
mode(tf_idf$groups)
tf_idf$groups <- as.character(tf_idf$groups)
tf_idf
#function to find the number decimal places
num.decimals <- function(x) {
stopifnot(class(x)=="numeric")
x <- sub("0+$","",x)
x <- sub("^.+[.]","",x)
nchar(x)
}
metric == "tf"
tf <- data.frame(tf) %>%
rename(freq=c)
row.names(tf) <- tf$word
#use only the top 1000 words
tf <- tf[1:500,]
tf_plot <- wordcloud2(data=tf, size = 0.7, shape = 'circle')
tf_plot
length(tf$freq)
#borrow freq list of 'tf'
magn_tf <- length(tf$freq)
#borrow freq list of 'tf'
magn_tf <- tf$freq
magn_tf
data.frame(tf_idf) %>%
rename(freq=tf_idf) %>%
select(word, freq) %>%
arrange(-freq, word) %>%
mutate(freq=round(freq, digits = 3))%>%
mutate(dec_place = num.decimals(freq))%>%
filter(!duplicated(word))%>%
group_by(freq) %>%
mutate(count=n())%>%
mutate(bigN = as.numeric(paste("1e-", dec_place, sep=""))) %>%#modify by adding values
mutate(number=1)%>%
mutate(incr = cumsum(number))%>%
mutate(bigN2 = as.numeric(paste(paste("1e-", dec_place, sep=""),incr, sep="")))%>%
mutate(differentiator = freq + bigN2) %>%
mutate(freq=differentiator)%>%
select(word, freq)
length(magn_tf)
tf_idf[1:length(magn_tf),]
data.frame(tf_idf[1:length(magn_tf),])
tf_idf <- data.frame(tf_idf[1:length(magn_tf),])
tf_idf
tf_idf$freq
#tf_idf <- tf_idf[1:500,]
tf_idf$freq <- magn_tf
tf_idf
tf_idf <- tokenize_series %>%
group_by(word, groups)%>%
summarise(c=n())%>%
bind_tf_idf(word, groups, c) %>%
arrange(desc(tf_idf))%>%
group_by(groups)%>%
#remove duplicate
filter(!duplicated(word))
mode(tf_idf$groups)
tf_idf$groups <- as.character(tf_idf$groups)
#function to find the number decimal places
num.decimals <- function(x) {
stopifnot(class(x)=="numeric")
x <- sub("0+$","",x)
x <- sub("^.+[.]","",x)
nchar(x)
}
#if(metric == "tf-idf"){
tf_idf <- data.frame(tf_idf) %>%
rename(freq=tf_idf) %>%
select(word, freq) %>%
arrange(-freq, word) %>%
mutate(freq=round(freq, digits = 3))%>%
mutate(dec_place = num.decimals(freq))%>%
filter(!duplicated(word))%>%
group_by(freq) %>%
mutate(count=n())%>%
mutate(bigN = as.numeric(paste("1e-", dec_place, sep=""))) %>%#modify by adding values
mutate(number=1)%>%
mutate(incr = cumsum(number))%>%
mutate(bigN2 = as.numeric(paste(paste("1e-", dec_place, sep=""),incr, sep="")))%>%
mutate(differentiator = freq + bigN2) %>%
mutate(freq=differentiator)%>%
select(word, freq)
tf_idf
data.frame(tf_idf[1:length(magn_tf),])
tf_idf <- data.frame(tf_idf[1:length(magn_tf),])
#tf_idf <- tf_idf[1:500,]
tf_idf$freq <- magn_tf
wordcloud2(data=tf_idf, size = 0.7, shape = 'circle')
library(opitools)
output <- word_imp(textdoc = policing_dtd, metric= "tf-idf",
words_to_filter= NULL)
output
#POLICING..
output <- word_imp(textdoc = policing_dtd, metric= "tf",
words_to_filter= NULL)
output
#POLICING..
output <- word_imp(textdoc = policing_dtd, metric= "tf",
words_to_filter= c("policing", "police"))
output
output <- word_imp(textdoc = policing_dtd, metric= "tf-idf",
words_to_filter= c("policing", "police"))
output
output
#REVIEWS
output <- word_imp(textdoc = reviews_dtd, metric= "tf",
words_to_filter= NULL)
output
output <- word_imp(textdoc = reviews_dtd, metric= "tf-idf",
words_to_filter= NULL)
output
#DEBATE
output <- word_imp(textdoc = debate_dtd, metric= "tf",
words_to_filter= NULL)
output
output <- word_imp(textdoc = debate_dtd, metric= "tf-idf",
words_to_filter= NULL)
output
#DEBATE
output <- word_imp(textdoc = debate_dtd, metric= "tf",
words_to_filter= NULL)
output
#REVIEWS
output <- word_imp(textdoc = reviews_dtd, metric= "tf",
words_to_filter= NULL)
output
#REVIEWS
output <- word_imp(textdoc = reviews_dtd, metric= "tf",
words_to_filter= c("station"))
output
output <- word_imp(textdoc = reviews_dtd, metric= "tf-idf",
words_to_filter= c("station"))
output
#DEBATE
output <- word_imp(textdoc = debate_dtd, metric= "tf",
words_to_filter= NULL)
output
#DEBATE
output <- word_imp(textdoc = debate_dtd, metric= "tf",
words_to_filter= c("trump","hillary" ))
output
output <- word_imp(textdoc = debate_dtd, metric= "tf-idf",
words_to_filter= NULL)
output
output <- word_imp(textdoc = reviews_dtd, metric= "tf-idf",
words_to_filter= c("station"))
output
#DEBATE
output <- word_imp(textdoc = debate_dtd, metric= "tf",
words_to_filter= c("trump","hillary" ))
output
output <- word_imp(textdoc = debate_dtd, metric= "tf-idf",
words_to_filter= NULL)
output
output <- word_imp(textdoc = debate_dtd, metric= "tf-idf",
output <- word_imp(textdoc = debate_dtd, metric= "tf-idf",
words_to_filter = c("test"))
output
output <- word_imp(textdoc = debate_dtd, metric= "tf-idf",
words_to_filter = c("test","youtube"))
output
output <- word_imp(textdoc = debate_dtd, metric= "tf-idf",
words_to_filter = c("test","youtube","morons"))
output
output <- word_imp(textdoc = debate_dtd, metric= "tf-idf",
words_to_filter = c(""))
output
#create new fake text document
set.seed(1000)
doc <- data.frame(text=sample(c("I love research because it is good!",
"I do not like research, it is time-consuming",
"I have no opinion on it"), size=50,
replace = TRUE, prob = c(0.5, 0.3, 0.2)))
#append one more column
doc2 <- data.frame(doc, ID=seq.int(nrow(doc)))
doc2
#small doc
set.seed(1000)
doc1 <- data.frame(text=sample(c("I love research because it is good!",
"I do not like research, it is time-consuming",
"I have no opinion on it"), size=7,
replace = TRUE, prob = c(0.5, 0.3, 0.2)))
#append one more column
doc1_ <- data.frame(doc1, ID=seq.int(nrow(doc1)))
doc1_
library(opitools)
library(opitools)
library(opitools)
textdoc = policing_dtd
theme_keys=covid_theme
metric = 1
fun = NULL
nsim = 99
nsim = 9
alternative="two.sided"
quiet=TRUE
library(purrr)
library(likert)
keywords <- text <- ID <- sentiment<-flush.console <-
desc <- asterisk <- comb <- NULL
if(!is.null(theme_keys)){
theme_keys <- data.frame(theme_keys)
}
#output holder
output <- list()
#check if randomization is too small
if(nsim < 99){
stop("Number of simulation (nsim) is too small!!")
}
if(nsim > 9999){
stop(paste("Consider specifying a smaller",
"number of simulations (nsim)!!", sep=" "))
}
if(is.null(theme_keys)){
stop(" 'theme_keys' parameter cannot be 'NULL'!! ")
}
#check any contradiction in tail comparison
if(metric == 1 & (alternative %in% c("less", "greater"))){
stop(paste("When parameter `metric = 1`, argument",
" `alternative` must be set as 'two.sided'!! "))
}
if(metric %in% c(2:4) & alternative == "two.sided"){
stop(paste('When parameter `metric =` ', metric,
", argument `two.sided` must be set as 'less'!! ", sep=""))
}
#format keywords
theme_keys <- data.frame(as.character(theme_keys[,1]))
colnames(theme_keys) <- "keys"
theme_keys <-
as.character(theme_keys %>% map_chr(~ str_c(., collapse = "|")))
#format text records
textdoc <- data.frame(textdoc[,1])
colnames(textdoc) <- c("text")
#separate `textdoc` into two:
#textdoc_keypresent': contains any of the keywords
#textdoc_keyabsent': contains no keywords
textdoc_keypresent <- data.frame(textdoc) %>%
filter(str_detect(text, theme_keys, negate=FALSE)) %>%
mutate(keywords = "present")
if(nrow(textdoc_keypresent)==0){
stop(paste("The text record contains NONE of",
"the secondary keywords!! Operation terminated!!", sep=" "))
}
textdoc_keyabsent <- data.frame(textdoc) %>%
filter(stringr::str_detect(text, theme_keys, negate=TRUE))%>%
mutate(keywords = "absent")
keywords <- text <- ID <- sentiment<-flush.console <-
desc <- asterisk <- comb <- NULL
if(!is.null(theme_keys)){
theme_keys <- data.frame(theme_keys)
}
#output holder
output <- list()
#check if randomization is too small
if(nsim < 99){
stop("Number of simulation (nsim) is too small!!")
}
if(nsim > 9999){
stop(paste("Consider specifying a smaller",
"number of simulations (nsim)!!", sep=" "))
}
nsim = 99
if(!is.null(theme_keys)){
theme_keys <- data.frame(theme_keys)
}
#output holder
output <- list()
#check if randomization is too small
if(nsim < 99){
stop("Number of simulation (nsim) is too small!!")
}
if(nsim > 9999){
stop(paste("Consider specifying a smaller",
"number of simulations (nsim)!!", sep=" "))
}
if(is.null(theme_keys)){
stop(" 'theme_keys' parameter cannot be 'NULL'!! ")
}
#check any contradiction in tail comparison
if(metric == 1 & (alternative %in% c("less", "greater"))){
stop(paste("When parameter `metric = 1`, argument",
" `alternative` must be set as 'two.sided'!! "))
}
if(metric %in% c(2:4) & alternative == "two.sided"){
stop(paste('When parameter `metric =` ', metric,
", argument `two.sided` must be set as 'less'!! ", sep=""))
}
#format keywords
theme_keys <- data.frame(as.character(theme_keys[,1]))
colnames(theme_keys) <- "keys"
theme_keys <-
as.character(theme_keys %>% map_chr(~ str_c(., collapse = "|")))
#format text records
textdoc <- data.frame(textdoc[,1])
colnames(textdoc) <- c("text")
library(stringr)
#format keywords
theme_keys <- data.frame(as.character(theme_keys[,1]))
colnames(theme_keys) <- "keys"
theme_keys <-
as.character(theme_keys %>% map_chr(~ str_c(., collapse = "|")))
#format text records
textdoc <- data.frame(textdoc[,1])
colnames(textdoc) <- c("text")
textdoc_keypresent <- data.frame(textdoc) %>%
filter(str_detect(text, theme_keys, negate=FALSE)) %>%
mutate(keywords = "present")
library(dplyr)
textdoc_keypresent <- data.frame(textdoc) %>%
filter(str_detect(text, theme_keys, negate=FALSE)) %>%
mutate(keywords = "present")
if(nrow(textdoc_keypresent)==0){
stop(paste("The text record contains NONE of",
"the secondary keywords!! Operation terminated!!", sep=" "))
}
textdoc_keyabsent <- data.frame(textdoc) %>%
filter(stringr::str_detect(text, theme_keys, negate=TRUE))%>%
mutate(keywords = "absent")
#combine and retain the IDs of text records
#for later joining
textdoc_comb <- rbind(textdoc_keypresent, textdoc_keyabsent)
#create seq_ID
textdoc_comb$ID <- seq.int(nrow(textdoc_comb))
textonly <- data.frame(textdoc_comb$text) #head(textonly)
colnames(textonly) <- "text"
#drop the large text field
textdoc_comb <- textdoc_comb %>%
select(c(keywords, ID))
#compute the OSD and opinion scores
obj_both <- opi_score(textonly, metric = metric, fun = fun)
#extract the score
observed_score <- as.numeric(gsub("\\%", "", obj_both$opiscore))
#extract the OSD
OSD <- obj_both$OSD
#join OSD and the with the pre-processed
OSD_joined <- textdoc_comb %>%
left_join(OSD) %>%
#filter records that do not contain sentiment words
filter(!is.na(sentiment)) %>%
arrange(desc(keywords), sentiment)%>%
select(ID, sentiment, keywords)
#prepare data for Likert plot
#filter neutral
likert_osd <- OSD_joined %>%
#filter(sentiment != "neutral") %>%
arrange(keywords, desc(sentiment)) %>%
select(-c(ID)) %>%
mutate(class = if_else(sentiment == "neutral",
paste("neutral", "", sep=""),
paste(paste("key",keywords, sep="."), sentiment, sep="_")))%>%
select(class)
#plot function here
#if(pplot == TRUE){
lik_p1 <- data.frame(likert_osd) %>% mutate_if(is.character,as.factor)
# Make list of ordered factor variables
out <- lapply(lik_p1, function(x) ordered(x,
levels = c("key.absent_positive", "key.absent_negative",
"neutral", "key.present_negative", "key.present_positive")))
#  Combine into data.frame
res <- do.call( data.frame , out )
# Build plot
likert.options(legend="Classes")
p <- likert(res)
title <- "Percentage proportion of classes"
#plot(p, center=3, centered=FALSE) + ggtitle(title)
pp <- likert.bar.plot(p, legend="Classes")
#terminate process if keyword fields
#does not include both 'present' and 'absent'
pres_abs <- unique(OSD_joined$keywords)
if(length(pres_abs) == 1){
stop(paste("The 'theme_keys' are either completely present",
"or absent in a sentiment class! The process terminated!!",
sep=" "))
}
#generate expected scores using `opi_sim` function
expected_scores <- opi_sim(osd_data = OSD_joined,
nsim=nsim,
metric = metric,
fun = fun,
quiet=quiet)
#check if there is contradiction in
#the alternative argument
if(observed_score > 0 & alternative == "less"){
flush.console()
print(paste("Warning: 'Observed score' is positive!!",
"'two.sided' criterion is utilized!"))
alternative <- "two.sided"
}
if(observed_score <= 0 & alternative == "greater"){
flush.console()
print(paste("Warning: 'Observed score' is negative!!",
"'two.sided' criterion is utilized!"))
alternative <- "two.sided"
}
#first, check the alternative argument
#second, check the sign of the observed score.
#third, compute the p-value
if(alternative == "two.sided"){
if(observed_score <= 0){
S <- expected_scores[which(expected_scores <= observed_score)]
S <- length(S)
}
if(observed_score > 0){
S <- expected_scores[which(expected_scores > observed_score)]
S <- length(S)
}
p <- round((S + 1)/(nsim + 1), digits = nchar(nsim))
}
if(alternative == "less"){
S <- expected_scores[which(expected_scores <= observed_score)]
S <- length(S)
p <- round((S + 1)/(nsim + 1), digits = nchar(nsim))
}
if(alternative == "greater"){
S <- expected_scores[which(expected_scores > observed_score)]
S <- length(S)
p <- round((S + 1)/(nsim + 1), digits = nchar(nsim))
}
#preparing final result
S_beat <- S
#significance level
#first, create a table of significance
#nsim=99
ci <- c(95, 97.5, 99, 99.9, 99.99)
v <- c(nsim, ((nsim + 1) - ((nsim + 1) * (ci/100))))
#remove decimal locations
v <- v[v >= 1]
p_loc <- round(v/(nsim+1), digits = nchar(nsim+1))
p_loc <- p_loc[order(p_loc)]
#create list of asterix
aster <- NULL
for(i in 3:1){ #i=3
n <- rep("*", i)
aster <- rbind(aster, paste(n, collapse='' ))
}
aster <- rbind(aster, "'")
p_loc <- data.frame(cbind(p_loc, aster))
colnames(p_loc) <- c("p_loc", "asterisk")
v <- c(v, 0)
v <- v[order(v)]
#where does S falls
int <- findInterval(S_beat, v)
signif <- p_loc[int,2]
signif <- paste(signif, collapse='')
#finally merge col
p_loc <- p_loc %>%
mutate(comb=paste(p_loc, asterisk, sep="")) %>%
select(comb)
p_loc <- p_loc[, "comb"]
#collate all results
output$test <- "Test of significance (Randomization testing)"
output$criterion <- alternative
output$exp_summary <- summary(expected_scores)
output$p_table <- knitr::kable(data.frame(cbind(observed_score,
S_beat, nsim, pvalue=(S+1)/(nsim+1),   signif)))
output$p_key <- rev(p_loc)
output$p_formula <- "(S_beat + 1)/(nsim + 1)"
output$plot <- pp
output
testthat::test_dir("tests")
library(goodpractice) #to check...
path <- "C:/Users/monsu/Documents/GitHub/opitools"
g <- gp(path, checks = all_checks()[3:16], quiet = FALSE)
g
path <- "C:/Users/monsu/Documents/GitHub/opitools"
g <- gp(path, checks = all_checks()[3:16], quiet = FALSE)
g
path <- "C:/Users/monsu/Documents/GitHub/opitools"
g <- gp(path, checks = all_checks()[3:16], quiet = FALSE)
g
library(opitools)
usethis::use_github_action()
library(usethis)
usethis::use_github_action()
use_github_actions()
usethis::use_github_action("test-coverage")
library(installr)
install.pandoc()
