mutate(dec_place = num.decimals(freq))%>%
filter(!duplicated(word))%>%
group_by(freq) %>%
mutate(count=n())%>%
mutate(bigN = as.numeric(paste("1e-", dec_place, sep=""))) %>%#modify by adding values
mutate(number=1)%>%
mutate(incr = cumsum(number))%>%
mutate(bigN2 = paste(bigN+incr))
data.frame(tf_idf) %>%
rename(freq=tf_idf) %>%
select(word, freq) %>%
arrange(-freq, word) %>%
mutate(freq=round(freq, digits = 10))%>%
mutate(dec_place = num.decimals(freq))%>%
filter(!duplicated(word))%>%
group_by(freq) %>%
mutate(count=n())%>%
mutate(bigN = as.numeric(paste("1e-", dec_place, sep=""))) %>%#modify by adding values
mutate(number=1)%>%
mutate(incr = cumsum(number))%>%
mutate(bigN2 = bigN+  paste(as.numeric(paste("1e-", dec_place, sep="")),incr, sep=""))
data.frame(tf_idf) %>%
rename(freq=tf_idf) %>%
select(word, freq) %>%
arrange(-freq, word) %>%
mutate(freq=round(freq, digits = 10))%>%
mutate(dec_place = num.decimals(freq))%>%
filter(!duplicated(word))%>%
group_by(freq) %>%
mutate(count=n())%>%
mutate(bigN = as.numeric(paste("1e-", dec_place, sep=""))) %>%#modify by adding values
mutate(number=1)%>%
mutate(incr = cumsum(number))%>%
mutate(bigN2 = bigN+  paste(paste("1e-", dec_place, sep=""),incr, sep=""))
data.frame(tf_idf) %>%
rename(freq=tf_idf) %>%
select(word, freq) %>%
arrange(-freq, word) %>%
mutate(freq=round(freq, digits = 10))%>%
mutate(dec_place = num.decimals(freq))%>%
filter(!duplicated(word))%>%
group_by(freq) %>%
mutate(count=n())%>%
mutate(bigN = as.numeric(paste("1e-", dec_place, sep=""))) %>%#modify by adding values
mutate(number=1)%>%
mutate(incr = cumsum(number))%>%
mutate(bigN2 = paste(paste("1e-", dec_place, sep=""),incr, sep=""))
data.frame(tf_idf) %>%
rename(freq=tf_idf) %>%
select(word, freq) %>%
arrange(-freq, word) %>%
mutate(freq=round(freq, digits = 10))%>%
mutate(dec_place = num.decimals(freq))%>%
filter(!duplicated(word))%>%
group_by(freq) %>%
mutate(count=n())%>%
mutate(bigN = as.numeric(paste("1e-", dec_place, sep=""))) %>%#modify by adding values
mutate(number=1)%>%
mutate(incr = cumsum(number))%>%
mutate(bigN2 = as.numeric(paste(paste("1e-", dec_place, sep=""),incr, sep="")))
data.frame(tf_idf) %>%
rename(freq=tf_idf) %>%
select(word, freq) %>%
arrange(-freq, word) %>%
mutate(freq=round(freq, digits = 10))%>%
mutate(dec_place = num.decimals(freq))%>%
filter(!duplicated(word))%>%
group_by(freq) %>%
mutate(count=n())%>%
mutate(bigN = as.numeric(paste("1e-", dec_place, sep=""))) %>%#modify by adding values
mutate(number=1)%>%
mutate(incr = cumsum(number))%>%
mutate(bigN2 = as.numeric(paste(paste("1e-", dec_place, sep=""),incr, sep="")))%>%
mutate(differentiator = freq + bigN2)
data.frame(tf_idf) %>%
rename(freq=tf_idf) %>%
select(word, freq) %>%
arrange(-freq, word) %>%
mutate(freq=round(freq, digits = 3))%>%
mutate(dec_place = num.decimals(freq))%>%
filter(!duplicated(word))%>%
group_by(freq) %>%
mutate(count=n())%>%
mutate(bigN = as.numeric(paste("1e-", dec_place, sep=""))) %>%#modify by adding values
mutate(number=1)%>%
mutate(incr = cumsum(number))%>%
mutate(bigN2 = as.numeric(paste(paste("1e-", dec_place, sep=""),incr, sep="")))%>%
mutate(differentiator = freq + bigN2)
library(opitools)
output <- word_importance(textdoc = policing_dtd, metric= "tf",
n_top=15, words_to_filter= NULL)
output
output <- word_importance(textdoc = reviews_dtd, metric= "tf-idf",
n_top=5, words_to_filter= NULL)
output
library(opitools)
output <- word_importance(textdoc = policing_dtd, metric= "tf",
n_top=15, words_to_filter= NULL)
output
output <- word_importance(textdoc = policing_dtd, metric= "tf-idf",
n_top=15, words_to_filter= NULL)
output
tf
tf_idf
library(opitools)
output <- word_importance(textdoc = policing_dtd, metric= "tf-idf",
n_top=15, words_to_filter= NULL)
output
output <- word_importance(textdoc = reviews_dtd, metric= "tf-idf",
n_top=5, words_to_filter= NULL)
library(opitools)
output <- word_imp(textdoc = policing_dtd, metric= "tf-idf",
n_top=15, words_to_filter= NULL)
output
tf
tf$c
tf <- data.frame(tf) %>%
rename(freq=c)
row.names(tf) <- tf$word
tf$freq
hist(tf$freq)
hist(tf-idf$freq)
hist(tfidf$freq)
hist(tf_idf$freq)
tf_idf
tf
library(opitools)
output <- word_imp(textdoc = policing_dtd, metric= "tf-idf",
n_top=15, words_to_filter= NULL)
output <- word_imp(textdoc = policing_dtd, metric= "tf-idf",
n_top=15, words_to_filter= NULL)
tf
magn_tf <- tf$freq
magn_tf
tf_idf
tf_idf
textdoc
words_to_filter=NULL
metric = "tf"
output <- list()
#global variables
aes <- freq<- arrange <- collapse <- coord_flip <- desc <- facet_wrap <- filter <- geom_bar <-
group_by <- labs <- mutate <- ntile <- rowname <- scale_alpha_discrete <-
scale_fill_brewer <- select <- stopwords <- text <- text2 <-
tf <- top_n <- ungroup <- word <- wordcloud2 <- img1 <- img2 <-img3 <-
img4 <- img5 <- img6 <- img7 <- img8 <- img9 <- img10 <- img11 <-
img12 <- img13 <- img14 <- img15 <- img16 <- img17 <- img18 <- img19 <-
img20 <- groups <- tibble <- dev.new <- gg_color_hue <- hcl <-
slice <- par <- scale_y_discrete <- scale_x_continuous <- dec_place <-
number <- incr <- bigN2 <- differentiator <-
theme_bw <- NULL
dim(textdoc)[1] < 20
dim(textdoc)[1] > 10000000
dim(textdoc)[1] > 10000000
length(dim(textdoc)) != 2 & (dim(textdoc)[2] != 1 | dim(textdoc)[2] != 2)
nr <- dim(textdoc)[1] #no. of rows
nc <- dim(textdoc)[2] #no. of columns
nc == 1
dat <- as.data.frame(textdoc)
#create interval to determine the number of arbitrary group
#to impose on the document.
no_of_grps <- c(5, 10, 15, 20)
#abitr_label <- data.frame(rbind(c(20, 200),c(201, 1000),
#c(1001, 10000), c(10001, 10000000)))
abit_label <- c(20, 200, 1000, 10000, 10000000)
#number of arbitrary groups
n_grp <- no_of_grps[findInterval(nr, abit_label)]
#determine where data length fall in the
series <- tibble()
series <- tibble(text = as.character(dat[,1]))%>%
tibble::rownames_to_column() #append rownames to the data
series$groups <- ntile(as.numeric(series$rowname), n_grp)
series <- series %>%
select(-c(rowname))
nc == 2
#tokenize
tokenize_series <- series %>%
group_by(groups)%>%
#paste(text, collapse = " ")
summarise(text2 = paste(text, collapse=" "))%>%
rename(text=text2)%>%
unnest_tokens(word, text) %>%
dplyr::filter(!word %in% words_to_filter)
#removing stopwords
tokenize_series <- tokenize_series[!tokenize_series$word %in% stopwords("english"),]
tokenize_series
#calculate 'tf' across all groups & 'tf_idf' value per groups
tf <- tokenize_series %>%
select(word)%>%
group_by(word)%>%
summarise(c=n())%>%
#bind_tf_idf(word, groups, c) %>%
arrange(desc(c))
tf
tf_idf <- tokenize_series %>%
group_by(word, groups)%>%
summarise(c=n())%>%
bind_tf_idf(word, groups, c) %>%
arrange(desc(tf_idf))%>%
group_by(groups)%>%
#remove duplicate
filter(!duplicated(word))
tf_idf
mode(tf_idf$groups)
tf_idf$groups <- as.character(tf_idf$groups)
num.decimals <- function(x) {
stopifnot(class(x)=="numeric")
x <- sub("0+$","",x)
x <- sub("^.+[.]","",x)
nchar(x)
}
tf <- data.frame(tf) %>%
rename(freq=c)
row.names(tf) <- tf$word
#use only the top 1000 words
tf <- tf[1:500,]
tf
tf_idf <- data.frame(tf_idf) %>%
rename(freq=tf_idf) %>%
select(word, freq) %>%
arrange(-freq, word) %>%
mutate(freq=round(freq, digits = 3))%>%
mutate(dec_place = num.decimals(freq))%>%
filter(!duplicated(word))%>%
group_by(freq) %>%
mutate(count=n())%>%
mutate(bigN = as.numeric(paste("1e-", dec_place, sep=""))) %>%#modify by adding values
mutate(number=1)%>%
mutate(incr = cumsum(number))%>%
mutate(bigN2 = as.numeric(paste(paste("1e-", dec_place, sep=""),incr, sep="")))%>%
mutate(differentiator = freq + bigN2) %>%
mutate(freq=differentiator)%>%
select(word, freq)
tf_idf
tf_idf[1:500,]
tf_idf <- tf_idf[1:500,]
magn_tf
tf
tf[1:500,]
tf <- tf[1:500,]
nrow(tf)
tf$freq
#borrow freq list of 'tf'
magn_tf <- tf$freq
magn_tf
tf_idf <- tf_idf[1:500,]
tf_idf
tf_idf$freq <- magn_tf
tf_idf
wordcloud2(data=tf_idf, size = 0.7, shape = 'circle')
library(opitools)
output <- word_imp(textdoc = policing_dtd, metric= "tf-idf",
n_top=15, words_to_filter= NULL)
output <- word_imp(textdoc = policing_dtd, metric= "tf-idf",
words_to_filter= NULL)
textdoc
metric
words_to_filter
output <- list()
#global variables
aes <- freq<- arrange <- collapse <- coord_flip <- desc <- facet_wrap <- filter <- geom_bar <-
group_by <- labs <- mutate <- ntile <- rowname <- scale_alpha_discrete <-
scale_fill_brewer <- select <- stopwords <- text <- text2 <-
tf <- top_n <- ungroup <- word <- wordcloud2 <- img1 <- img2 <-img3 <-
img4 <- img5 <- img6 <- img7 <- img8 <- img9 <- img10 <- img11 <-
img12 <- img13 <- img14 <- img15 <- img16 <- img17 <- img18 <- img19 <-
img20 <- groups <- tibble <- dev.new <- gg_color_hue <- hcl <-
slice <- par <- scale_y_discrete <- scale_x_continuous <- dec_place <-
number <- incr <- bigN2 <- differentiator <-
theme_bw <- NULL
dim(textdoc)[1] < 20
nr <- dim(textdoc)[1] #no. of rows
nc <- dim(textdoc)[2] #no. of columns
nr
nc
nc == 1
dat <- as.data.frame(textdoc)
dat
#create interval to determine the number of arbitrary group
#to impose on the document.
no_of_grps <- c(5, 10, 15, 20)
#abitr_label <- data.frame(rbind(c(20, 200),c(201, 1000),
#c(1001, 10000), c(10001, 10000000)))
abit_label <- c(20, 200, 1000, 10000, 10000000)
#number of arbitrary groups
n_grp <- no_of_grps[findInterval(nr, abit_label)]
n_grp
#determine where data length fall in the
series <- tibble()
series <- tibble(text = as.character(dat[,1]))%>%
tibble::rownames_to_column() #append rownames to the data
series$groups <- ntile(as.numeric(series$rowname), n_grp)
series <- series %>%
select(-c(rowname))
nc == 2
#tokenize
tokenize_series <- series %>%
group_by(groups)%>%
#paste(text, collapse = " ")
summarise(text2 = paste(text, collapse=" "))%>%
rename(text=text2)%>%
unnest_tokens(word, text) %>%
dplyr::filter(!word %in% words_to_filter)
tokenize_series
#removing stopwords
tokenize_series <- tokenize_series[!tokenize_series$word %in% stopwords("english"),]
tokenize_series
#calculate 'tf' across all groups & 'tf_idf' value per groups
tf <- tokenize_series %>%
select(word)%>%
group_by(word)%>%
summarise(c=n())%>%
#bind_tf_idf(word, groups, c) %>%
arrange(desc(c))
tf
tf_idf <- tokenize_series %>%
group_by(word, groups)%>%
summarise(c=n())%>%
bind_tf_idf(word, groups, c) %>%
arrange(desc(tf_idf))%>%
group_by(groups)%>%
#remove duplicate
filter(!duplicated(word))
tf_idf
mode(tf_idf$groups)
tf_idf$groups <- as.character(tf_idf$groups)
tf_idf
#function to find the number decimal places
num.decimals <- function(x) {
stopifnot(class(x)=="numeric")
x <- sub("0+$","",x)
x <- sub("^.+[.]","",x)
nchar(x)
}
metric == "tf"
tf <- data.frame(tf) %>%
rename(freq=c)
row.names(tf) <- tf$word
#use only the top 1000 words
tf <- tf[1:500,]
tf_plot <- wordcloud2(data=tf, size = 0.7, shape = 'circle')
tf_plot
length(tf$freq)
#borrow freq list of 'tf'
magn_tf <- length(tf$freq)
#borrow freq list of 'tf'
magn_tf <- tf$freq
magn_tf
data.frame(tf_idf) %>%
rename(freq=tf_idf) %>%
select(word, freq) %>%
arrange(-freq, word) %>%
mutate(freq=round(freq, digits = 3))%>%
mutate(dec_place = num.decimals(freq))%>%
filter(!duplicated(word))%>%
group_by(freq) %>%
mutate(count=n())%>%
mutate(bigN = as.numeric(paste("1e-", dec_place, sep=""))) %>%#modify by adding values
mutate(number=1)%>%
mutate(incr = cumsum(number))%>%
mutate(bigN2 = as.numeric(paste(paste("1e-", dec_place, sep=""),incr, sep="")))%>%
mutate(differentiator = freq + bigN2) %>%
mutate(freq=differentiator)%>%
select(word, freq)
length(magn_tf)
tf_idf[1:length(magn_tf),]
data.frame(tf_idf[1:length(magn_tf),])
tf_idf <- data.frame(tf_idf[1:length(magn_tf),])
tf_idf
tf_idf$freq
#tf_idf <- tf_idf[1:500,]
tf_idf$freq <- magn_tf
tf_idf
tf_idf <- tokenize_series %>%
group_by(word, groups)%>%
summarise(c=n())%>%
bind_tf_idf(word, groups, c) %>%
arrange(desc(tf_idf))%>%
group_by(groups)%>%
#remove duplicate
filter(!duplicated(word))
mode(tf_idf$groups)
tf_idf$groups <- as.character(tf_idf$groups)
#function to find the number decimal places
num.decimals <- function(x) {
stopifnot(class(x)=="numeric")
x <- sub("0+$","",x)
x <- sub("^.+[.]","",x)
nchar(x)
}
#if(metric == "tf-idf"){
tf_idf <- data.frame(tf_idf) %>%
rename(freq=tf_idf) %>%
select(word, freq) %>%
arrange(-freq, word) %>%
mutate(freq=round(freq, digits = 3))%>%
mutate(dec_place = num.decimals(freq))%>%
filter(!duplicated(word))%>%
group_by(freq) %>%
mutate(count=n())%>%
mutate(bigN = as.numeric(paste("1e-", dec_place, sep=""))) %>%#modify by adding values
mutate(number=1)%>%
mutate(incr = cumsum(number))%>%
mutate(bigN2 = as.numeric(paste(paste("1e-", dec_place, sep=""),incr, sep="")))%>%
mutate(differentiator = freq + bigN2) %>%
mutate(freq=differentiator)%>%
select(word, freq)
tf_idf
data.frame(tf_idf[1:length(magn_tf),])
tf_idf <- data.frame(tf_idf[1:length(magn_tf),])
#tf_idf <- tf_idf[1:500,]
tf_idf$freq <- magn_tf
wordcloud2(data=tf_idf, size = 0.7, shape = 'circle')
library(opitools)
output <- word_imp(textdoc = policing_dtd, metric= "tf-idf",
words_to_filter= NULL)
output
#POLICING..
output <- word_imp(textdoc = policing_dtd, metric= "tf",
words_to_filter= NULL)
output
#POLICING..
output <- word_imp(textdoc = policing_dtd, metric= "tf",
words_to_filter= c("policing", "police"))
output
output <- word_imp(textdoc = policing_dtd, metric= "tf-idf",
words_to_filter= c("policing", "police"))
output
output
#REVIEWS
output <- word_imp(textdoc = reviews_dtd, metric= "tf",
words_to_filter= NULL)
output
output <- word_imp(textdoc = reviews_dtd, metric= "tf-idf",
words_to_filter= NULL)
output
#DEBATE
output <- word_imp(textdoc = debate_dtd, metric= "tf",
words_to_filter= NULL)
output
output <- word_imp(textdoc = debate_dtd, metric= "tf-idf",
words_to_filter= NULL)
output
#DEBATE
output <- word_imp(textdoc = debate_dtd, metric= "tf",
words_to_filter= NULL)
output
#REVIEWS
output <- word_imp(textdoc = reviews_dtd, metric= "tf",
words_to_filter= NULL)
output
#REVIEWS
output <- word_imp(textdoc = reviews_dtd, metric= "tf",
words_to_filter= c("station"))
output
output <- word_imp(textdoc = reviews_dtd, metric= "tf-idf",
words_to_filter= c("station"))
output
#DEBATE
output <- word_imp(textdoc = debate_dtd, metric= "tf",
words_to_filter= NULL)
output
#DEBATE
output <- word_imp(textdoc = debate_dtd, metric= "tf",
words_to_filter= c("trump","hillary" ))
output
output <- word_imp(textdoc = debate_dtd, metric= "tf-idf",
words_to_filter= NULL)
output
output <- word_imp(textdoc = reviews_dtd, metric= "tf-idf",
words_to_filter= c("station"))
output
#DEBATE
output <- word_imp(textdoc = debate_dtd, metric= "tf",
words_to_filter= c("trump","hillary" ))
output
output <- word_imp(textdoc = debate_dtd, metric= "tf-idf",
words_to_filter= NULL)
output
output <- word_imp(textdoc = debate_dtd, metric= "tf-idf",
output <- word_imp(textdoc = debate_dtd, metric= "tf-idf",
words_to_filter = c("test"))
output
output <- word_imp(textdoc = debate_dtd, metric= "tf-idf",
words_to_filter = c("test","youtube"))
output
output <- word_imp(textdoc = debate_dtd, metric= "tf-idf",
words_to_filter = c("test","youtube","morons"))
output
output <- word_imp(textdoc = debate_dtd, metric= "tf-idf",
words_to_filter = c(""))
output
#create new fake text document
set.seed(1000)
doc <- data.frame(text=sample(c("I love research because it is good!",
"I do not like research, it is time-consuming",
"I have no opinion on it"), size=50,
replace = TRUE, prob = c(0.5, 0.3, 0.2)))
#append one more column
doc2 <- data.frame(doc, ID=seq.int(nrow(doc)))
doc2
#small doc
set.seed(1000)
doc1 <- data.frame(text=sample(c("I love research because it is good!",
"I do not like research, it is time-consuming",
"I have no opinion on it"), size=7,
replace = TRUE, prob = c(0.5, 0.3, 0.2)))
#append one more column
doc1_ <- data.frame(doc1, ID=seq.int(nrow(doc1)))
doc1_
library(opitools)
library(opitools)
