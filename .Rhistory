#ggplot(aes(word, tf_idf, fill = Regions)) +
ggplot(aes(word, tf, fill = groups)) +
geom_bar(stat = "identity", alpha = .8, show.legend = FALSE) +
labs(title = "",
x = NULL, y = "tf-idf") +
facet_wrap(~groups, ncol = 2, scales = "free") +
scale_alpha_discrete(range = c(0.4,1)) +
scale_fill_brewer(palette = "Set1"
, name = "groups")+
coord_flip()
}
#if false, do nothing
if(showplot == FALSE){
plt <- "Plot disabled"
#do nothings
}
output[2] <- plt
return(output)
}
tf_idf(textdoc = tweets, n_top=10, showplot=FALSE)
traceback()
textdoc
output <- list()
#global variables
aes <- arrange <- collapse <- coord_flip <- desc <- facet_wrap <- filter <- geom_bar <- group
group_by <- labs <- mutate <- ntile <- rowname <- scale_alpha_discrete <-
scale_fill_brewer <- select <- stopwords <- text <- tf <- top_n <- tweets <- ungroup <- word <-
group <- dev.new <- NULL
dim(textdoc)[1] < 20
dim(textdoc)[1] > 10000000
length(dim(textdoc)) != 2 & (dim(textdoc)[2] != 1 | dim(textdoc)[2] != 2)
nr <- dim(textdoc)[1] #no. of rows
nc <- dim(textdoc)[2] #no. of columns
nc == 1
nc == 2
dat <- as.data.frame(textdoc)
series <- tibble()
series <- tibble(text = as.character(dat[,1]), group = dat[,2])
groups <- unique(as.character(dat[,2]))
groups
length(groups) <= 3
tf_idf <- function(textdoc, n_top=10, showplot=FALSE){
output <- list()
#global variables
aes <- arrange <- collapse <- coord_flip <- desc <- facet_wrap <- filter <- geom_bar <- group
group_by <- labs <- mutate <- ntile <- rowname <- scale_alpha_discrete <-
scale_fill_brewer <- select <- stopwords <- text <- tf <- top_n <- tweets <- ungroup <- word <-
group <- dev.new <- NULL
#if the length of document is too small
if(dim(textdoc)[1] < 20){
stop(paste("Length of document is too small!!",
"The minimum document length is 20!",
"Process terminated!!", sep = " "))
}
#if the length of document is too large
if(dim(textdoc)[1] > 10000000){
stop(paste("Length of document is too large!!",
"The maximum document length is 10 million records!!",
"Process terminated!!", sep = " "))
}
#if a dataframe of more than two column is supplied
if(length(dim(textdoc)) != 2 & (dim(textdoc)[2] != 1 | dim(textdoc)[2] != 2)){
stop("Input data needs to be a dataframe containing 1 or 2 columns")
}
nr <- dim(textdoc)[1] #no. of rows
nc <- dim(textdoc)[2] #no. of columns
#if there are no groupings in the text document,
#create an arbitrary group
if(nc == 1){
dat <- as.data.frame(textdoc)
#create interval to determine number of arbitrary group
#to impose on a document.
no_of_grps <- c(5, 10, 15, 20)
abit_label <- data.frame(rbind(c(20, 200),c(201, 1000),
c(1001, 10000), c(10001, 10000000)))
#join
abit_label <- cbind(abit_label, no_of_grps)
#determine where data length fall in the
#intervals
data <- data.frame(ID = 1:4,
time.s = c(1,1,2,2),
time.e = c(2,4,3,4))
n_grp <- abit_label[which(data.table::between(nr,
abit_label$X1, abit_label$X2))
,3]
series <- tibble()
series <- tibble(text = as.character(dat[,1]))%>%
tibble::rownames_to_column() #append rownames to the data
series$group <- ntile(as.numeric(series$rowname), n_grp)
series <- series %>%
select(-c(rowname))
}
#if there are groupings in the document,
#check that there are at least 20 text records
#per group
#groups
if(nc == 2){
dat <- as.data.frame(textdoc)
series <- tibble()
series <- tibble(text = as.character(dat[,1]), group = dat[,2])
groups <- unique(as.character(dat[,2]))
#if groups are less than 4, stop
#if greater or equal to 4, okay
if(length(groups) <= 3){
stop(paste("The number of groups in the text document",
"should be greater than 3!!",
"And ensure that there are at least 20 text",
"records per group", sep=" "))
}
}
#tokenize
tokenize_series <- series %>%
group_by(groups)%>%
collapse(text, sep= " ")%>%
unnest_tokens(word, text) #%>%
#removing stopwords
tokenize_series <- tokenize_series[!tokenize_series$word %in% stopwords("english"),]
#calculate tf_idf value per groups
tf_idf <- tokenize_series %>%
group_by(word, groups)%>%
mutate(c=n())%>%
bind_tf_idf(word, groups, c) %>%
arrange(desc(tf_idf))%>%
group_by(groups)%>%
#remove duplicate
filter(!duplicated(word))
#tf_idf[which(tf_idf$group==7),]
mode(tf_idf$groups)
tf_idf$groups <- as.character(tf_idf$groups)
#print results
output[1] <- list(tf_idf)
#show plot
if(showplot == TRUE){
dev.new()
plt <- tf_idf %>%
arrange(desc(tf_idf)) %>%
#arrange(desc(tf)) %>%
mutate(
groups = factor(groups, levels = unique(groups))) %>%
group_by(group) %>%
#top_n(10, wt = tf_idf) %>%
top_n(n_top, wt = tf_idf) %>%
ungroup() %>%
#ggplot(aes(word, tf_idf, fill = Regions)) +
ggplot(aes(word, tf, fill = groups)) +
geom_bar(stat = "identity", alpha = .8, show.legend = FALSE) +
labs(title = "",
x = NULL, y = "tf-idf") +
facet_wrap(~groups, ncol = 2, scales = "free") +
scale_alpha_discrete(range = c(0.4,1)) +
scale_fill_brewer(palette = "Set1"
, name = "groups")+
coord_flip()
flush.console()
print(plt)
}
#if false, do nothing
if(showplot == FALSE){
#do nothings
}
output[2] <- plt
return(output)
}
tf_idf(textdoc = tweets, n_top=10, showplot=FALSE)
output <- list()
#global variables
aes <- arrange <- collapse <- coord_flip <- desc <- facet_wrap <- filter <- geom_bar <- group
group_by <- labs <- mutate <- ntile <- rowname <- scale_alpha_discrete <-
scale_fill_brewer <- select <- stopwords <- text <- tf <- top_n <- tweets <- ungroup <- word <-
group <- dev.new <- NULL
dim(textdoc)[1] < 20
dim(textdoc)
dim(textdoc)[1]
textdoc = tweets
textdoc
tweets
tf_idf <- function(textdoc, n_top=10, showplot=FALSE){
output <- list()
#global variables
aes <- arrange <- collapse <- coord_flip <- desc <- facet_wrap <- filter <- geom_bar <- group
group_by <- labs <- mutate <- ntile <- rowname <- scale_alpha_discrete <-
scale_fill_brewer <- select <- stopwords <- text <- tf <- top_n <- tweets <- ungroup <- word <-
groups <- dev.new <- NULL
#if the length of document is too small
if(dim(textdoc)[1] < 20){
stop(paste("Length of document is too small!!",
"The minimum document length is 20!",
"Process terminated!!", sep = " "))
}
#if the length of document is too large
if(dim(textdoc)[1] > 10000000){
stop(paste("Length of document is too large!!",
"The maximum document length is 10 million records!!",
"Process terminated!!", sep = " "))
}
#if a dataframe of more than two column is supplied
if(length(dim(textdoc)) != 2 & (dim(textdoc)[2] != 1 | dim(textdoc)[2] != 2)){
stop("Input data needs to be a dataframe containing 1 or 2 columns")
}
nr <- dim(textdoc)[1] #no. of rows
nc <- dim(textdoc)[2] #no. of columns
#if there are no groupings in the text document,
#create an arbitrary group
if(nc == 1){
dat <- as.data.frame(textdoc)
#create interval to determine number of arbitrary group
#to impose on a document.
no_of_grps <- c(5, 10, 15, 20)
abit_label <- data.frame(rbind(c(20, 200),c(201, 1000),
c(1001, 10000), c(10001, 10000000)))
#join
abit_label <- cbind(abit_label, no_of_grps)
#determine where data length fall in the
#intervals
data <- data.frame(ID = 1:4,
time.s = c(1,1,2,2),
time.e = c(2,4,3,4))
n_grp <- abit_label[which(data.table::between(nr,
abit_label$X1, abit_label$X2))
,3]
series <- tibble()
series <- tibble(text = as.character(dat[,1]))%>%
tibble::rownames_to_column() #append rownames to the data
series$groups <- ntile(as.numeric(series$rowname), n_grp)
series <- series %>%
select(-c(rowname))
}
#if there are groupings in the document,
#check that there are at least 20 text records
#per group
#groups
if(nc == 2){
dat <- as.data.frame(textdoc)
series <- tibble()
series <- tibble(text = as.character(dat[,1]), groups = dat[,2])
groups <- unique(as.character(dat[,2]))
#if groups are less than 4, stop
#if greater or equal to 4, okay
if(length(groups) <= 3){
stop(paste("The number of groups in the text document",
"should be greater than 3!!",
"And ensure that there are at least 20 text",
"records per group", sep=" "))
}
}
#tokenize
tokenize_series <- series %>%
group_by(groups)%>%
collapse(text, sep= " ")%>%
unnest_tokens(word, text) #%>%
#removing stopwords
tokenize_series <- tokenize_series[!tokenize_series$word %in% stopwords("english"),]
#calculate tf_idf value per groups
tf_idf <- tokenize_series %>%
group_by(word, groups)%>%
mutate(c=n())%>%
bind_tf_idf(word, groups, c) %>%
arrange(desc(tf_idf))%>%
group_by(groups)%>%
#remove duplicate
filter(!duplicated(word))
#tf_idf[which(tf_idf$group==7),]
mode(tf_idf$groups)
tf_idf$groups <- as.character(tf_idf$groups)
#print results
output[1] <- list(tf_idf)
#show plot
if(showplot == TRUE){
dev.new()
plt <- tf_idf %>%
arrange(desc(tf_idf)) %>%
#arrange(desc(tf)) %>%
mutate(
groups = factor(groups, levels = unique(groups))) %>%
group_by(groups) %>%
#top_n(10, wt = tf_idf) %>%
top_n(n_top, wt = tf_idf) %>%
ungroup() %>%
#ggplot(aes(word, tf_idf, fill = Regions)) +
ggplot(aes(word, tf, fill = groups)) +
geom_bar(stat = "identity", alpha = .8, show.legend = FALSE) +
labs(title = "",
x = NULL, y = "tf-idf") +
facet_wrap(~groups, ncol = 2, scales = "free") +
scale_alpha_discrete(range = c(0.4,1)) +
scale_fill_brewer(palette = "Set1"
, name = "groups")+
coord_flip()
flush.console()
print(plt)
}
#if false, do nothing
if(showplot == FALSE){
#do nothings
}
output[2] <- plt
return(output)
}
tf_idf(textdoc = tweets, n_top=10, showplot=FALSE)
tweets
library(opitools)
dim(policing_otd)
word_distrib(textdoc = policing_otd)
word_distrib(textdoc = policing_otd)
tweets
library(opitools)
dim(tweets)
tweets
rm(list=ls())
tweets
plt = word_distrib(textdoc = tweets)
library(opitools)
library(opitools)
library(opitools)
library(roxygen2)
library(opitools)
library(opitools)
library(opitools)
library(opitools)
library(opitools)
dim(tweets)
tweets_dat <- tweets[,1]
plt = word_distrib(textdoc = tweets)
tweets
tweets[,1]
tweets_dat <- tweets[,1]
word_distrib(textdoc = tweets_dat)
dim(tweets_dat)
tweets_dat <- tweets[,1]
tweets_dat
length(tweets_dat)
data.frame(tweets_dat)
dim(data.frame(tweets_dat))
dim(as.data.frame(tweets_dat))
x <- 2:18
v <- c(5, 10, 15) # create two bins [5,10) and [10,15)
x
v
findInterval(x, v)
x <- 2
v <- c(5, 10, 15)
cbind(x, findInterval(x, v))
nr
nr = 1001
no_of_grps <- c(5, 10, 15, 20)
#abit_label <- data.frame(rbind(c(20, 200),c(201, 1000),
#c(1001, 10000), c(10001, 10000000)))
abit_label <- c(20, 200, 1000, 10000, 10000000)
abit_label
findInterval(nr, abit_label)
#join
n_grp <- no_of_grps[findInterval(nr, abit_label)]
n_grp
library(opitools)
system("R CMD Rd2pdf . --title = Package opitools --output=./opitools_user_manual.pdf --force --no-clean --internals")
library(tm)
#Create a vector containing only the text
text <- data$text
library(wordcloud)
library(RColorBrewer)
install.packages("wordcloud2)
library(wordcloud2)
install.packages("tm")
library(tm)
#Create a vector containing only the text
text <- data$text
# Create a corpus
docs <- Corpus(VectorSource(text))
library(wordcloud2)
library(tm)
#Create a vector containing only the text
text <- data$text
library(wordcloud2)
library(tm)
#Create a vector containing only the text
text <- data$text
library(tm)
library(opitools)
covid_keys
mode(covid_keys)
mode(osd_data)
dim(osd_data)
mode(policing_otd)
dim(policing_otd)
mode(tweets)
dim(tweets)
library(testthat)
testthat::test_dir("tests")
library(opitools)
library(testthat)
testthat::test_dir(“opitools”)
testthat::test_dir(“opitools”)
testthat::test_dir("opitools")
testthat::test_dir("test")
testthat::test_dir("tests")
testthat::test_dir("tests")
head(covid_keys)
head(osd_data)
head(policing_otd)
dim(policing_otd)
head(tweets)
library(wordcloud2)
head(demoFreq)
tweets
dat <- list(tweets[,1])
dat
dim(dat)
dat <- list(data.frame(tweets[,1]))
head(dat)
series <- tibble()
library(tibble)
series <- tibble()
tibble(text = as.character(unlist(dat)))%>%
unnest_tokens(word, text)
library(tidytext)
library(tibble)
tibble(text = as.character(unlist(dat)))%>%
unnest_tokens(word, text)
tibble(text = as.character(unlist(dat)))%>%
unnest_tokens(word, text)%>% #tokenize
dplyr::select(everything())
tokenized <- tibble(text = as.character(unlist(dat)))%>%
unnest_tokens(word, text)%>% #tokenize
dplyr::select(everything())
series <- tokenized
series
library(tm)
head(series)
stopwords("english")
nrow(series)
#removing stopwords
tokenize_series <- series[!series$word %in% stopwords("english"),]
nrow(tokenize_series)
head(tokenize_series)
tokenize_series %>%
dplyr::count(word, sort = TRUE) %>%
dplyr::ungroup()
tokenize_series %>%
dplyr::count(word, sort = TRUE) %>%
dplyr::ungroup() %>%
dplyr::mutate(len=nchar(word))
tokenize_series %>%
dplyr::count(word, sort = TRUE) %>%
dplyr::ungroup() %>%
dplyr::mutate(len=nchar(word)) %>% #remove words char > 2
dplyr::filter(len > 2)
doc_words <- tokenize_series %>%
dplyr::count(word, sort = TRUE) %>%
dplyr::ungroup() %>%
dplyr::mutate(len=nchar(word)) %>% #remove words char > 2
dplyr::filter(len > 2)
doc_words %>%
summarise(total = sum(n))
word(doc_words)
head(doc_words)
row.names(doc_words) <- doc_words$word
doc_words
doc_words <- tokenize_series %>%
dplyr::count(word, sort = TRUE) %>%
dplyr::ungroup() %>%
dplyr::mutate(len=nchar(word)) %>% #remove words char > 2
dplyr::filter(len > 2)%>%
data.frame()
doc_words
row.names(doc_words) <- doc_words$word
head(doc_words)
head(demoFreq)
head(doc_words)
doc_words <- tokenize_series %>%
dplyr::count(word, sort = TRUE) %>%
dplyr::ungroup() %>%
dplyr::mutate(len=nchar(word)) %>% #remove words char > 2
dplyr::filter(len > 2)%>%
data.frame() %>%
dplyr::rename(freq=n)%>%
dplyr::select(-c(len))
head(doc_words)
row.names(doc_words) <- doc_words$word
head(doc_words)
wordcloud2(data=doc_words, size = 0.7, shape = 'pentagon')
doc_words <- tokenize_series %>%
dplyr::count(word, sort = TRUE) %>%
dplyr::ungroup() %>%
dplyr::mutate(len=nchar(word)) %>% #remove words char > 2
dplyr::filter(len > 2)%>%
data.frame() %>%
dplyr::rename(freq=n)%>%
dplyr::select(-c(len))%>%
filter(!word %>% c("police", "policing"))
#compute term frequencies
doc_words <- tokenize_series %>%
dplyr::count(word, sort = TRUE) %>%
dplyr::ungroup() %>%
dplyr::mutate(len=nchar(word)) %>% #remove words char > 2
dplyr::filter(len > 2)%>%
data.frame() %>%
dplyr::rename(freq=n)%>%
dplyr::select(-c(len))%>%
dplyr::filter(!word %in% c("police", "policing"))
row.names(doc_words) <- doc_words$word
head(doc_words)
wordcloud2(data=doc_words, size = 0.7, shape = 'pentagon')
nrow(doc_words)
wordcloud2(data=doc_words[1:1000,], size = 0.7, shape = 'pentagon')
wordcloud2(data=doc_words[1:1000,], size = 0.7, shape = 'pentagon')
wordcloud2(data=doc_words[1:1000,], size = 0.7, shape = 'pentagon')
wordcloud2(data=doc_words[1:1000,], size = 0.7, shape = 'pentagon')
dev.new()
dev.new()
wordcloud2(data=doc_words[1:1000,], size = 0.7, shape = 'pentagon')
wordcloud2(data=doc_words[1:1000,], size = 0.7, shape = 'pentagon')
wordcloud2(data=doc_words[1:1000,], size = 0.7, shape = 'pentagon')
