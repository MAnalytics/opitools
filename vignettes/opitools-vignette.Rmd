---
title: "Assessing the impacts of COVID-19 pandemic on public opinion concerning policing using Twitter data - A demonstration using `'Opitools'` package"

author: |
  | `Author:`
  | `Adepeju, M.`
  | `Big Data Centre, Manchester Metropolitan University, Manchester, M15 6BH`
  
date: |
  | `Date:`
  | ``r Sys.Date()``

output:
  rmarkdown::html_vignette
  
#dev: png
#output:
  #word_document: default
  #always_allow_html: yes
#  pdf_document: default
always_allow_html: yes
#fig_caption: yes
bibliography: references.bib

abstract: The lack of tools for analysing cross-impact of opinions expressed amongst multiple subjects, within a text document, facilitate the development of `'opitool'` package. For instance, given a collection of tweets on a specific subject A, a researcher may want to assess whether the opinion expressed on subject A in relation to another (secondary) subject B has significantly impacted the overall opinion from the text document. For a real life example, we may want to examine if public opinion expressed concerning policing (as subject A) has been significantly impacted by the public concerns for COVID-19 pandemic (as subject B) (see Adepeju and Jimoh, 2021). This document describes how the aforementioned analysis can be completed using the `opitools` package.


vignette: >
  %\VignetteIndexEntry{A guide to measuring long-term inequality in the exposure to crime at micro-area levels using 'Akmedoids' package}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

<style type="text/css">

h1.title {
  font-size: 26px;
  line-height: 130%;
  color: Black;
  text-align: center;
}

h2.subtitle {
  font-size: 13px;
  line-height: 120%;
  color: Black;
  text-align: center;
}

h4.author { /* Header 4 - and the author and data headers use this too  */
  font-size: 17px;
  font-family: "Arial";
  color: Black;
  text-align: center;
}
h4.date { /* Header 4 - and the author and data headers use this too  */
  font-size: 17px;
  font-family: "Arial", Times, serif;
  color: Black;
  text-align: center;
}

h4.abstract { /* Header 4 - and the author and data headers use this too  */
  font-size: 10px;
  font-family: "Arial", Times, serif;
  color: black;
  text-align: center;
}

h4.institute{ /* Header 4 - and the author and data headers use this too  */
  font-size: 10px;
  font-family: "Arial", Times, serif;
  color: black;
  text-align: center;
}

body, td {
   font-size: 14px;
}
code.r{
  font-size: 13px;
}
pre {
  font-size: 13px
}
h1 { /* Header 1 */
  font-size: 16px;
  color: DarkBlue;
}
h2 { /* Header 2 */
    font-size: 16px;
  color: DarkBlue;
}
h3 { /* Header 3 */
  font-size: 15px;
  font-family: "Times New Roman", Times, serif;
  color: DarkBlue;

</style>

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r functions, include=FALSE}
# A function for captioning and referencing images
fig <- local({
    i <- 0
    ref <- list()
    list(
        cap=function(refName, text) {
            i <<- i + 1
            ref[[refName]] <<- i
            paste("Figure ", i, ": ", text, sep="")
        },
        ref=function(refName) {
            ref[[refName]]
        })
})
```



# Introduction

The `opitools` is an opinion analytical toolset designed for assessing cross-impacts of opinion expressed on multiple subjects in an opinion-based text documents (OTD) from a social media platform. An OTD (input as `textdoc`) should composed of individual text records on a specific subject (A). An example of an OTD is a collection of Twitter posts concerning a specific topic or hashtag. Any other subjects referenced in relation to the primary subject A can be referred to as secondary subjects, and they can be identified through the keywords used in the text records.In the article [@Adepeju2021], we described how to deploy `opitools` in order to answer a real-life research question, such as 'what are the impacts of `COVID-19 pandemic` (secondary subject) on the public opinion concerning policing (primary subject) across England and Wales?' The `opitools` may be used to answer similar questions relating to many public organisations in order to unravelling important issues that may be driving confidence and trust in relation to their services. 



# Downloading Twitter data

The `rtweet` package [@Kearney2019] is one of the R packages that provide access to the Twitter API for data download. The code section below can be used to download tweets for a pre-defined geographical coverage (lat:'53.805,long:-4.242,radius: 350mi') within the last seven days (free download). We will be downloading tweets relating to 'policing'. Thus, I define the search words as {"`police`", "`policing`", "`law enforcement`"} Note: A user need to first secure access to Twitter developer platform (from [here](https://developer.twitter.com/en/apply-for-access)), then follow instructions on this [page](https://developer.twitter.com/en/docs/twitter-api/getting-started/guide) on how to obtain a set of tokens (keys) required connect to the Twitter API.

### Working directory

Set a local directory:

```{r, message=FALSE, eval=FALSE}
WORKING_DIR <- 'C:/R/Github/JGIS_Policing_COVID-19'

#setting working directory
setwd(WORKING_DIR)
```

### Installing libraries, essential function, and define tokens

Free Twitter developer accounts have a restriction of 18,000 tweets per 15 minutes, otherwise a user may loose access (temporarily) to download data. Therefore, it is important to wait for 15 minutes after every 18,000 tweets download. Run the codes below to install the necessary libraries and the `waitFun` function to help with the restriction. 

```{r, message=FALSE, eval=FALSE}

library(opitools)
library(rtweet)
library(twitteR) #for setting up Twitter authorization

#Run function 
waitFun <- function(x){
  p1 <- proc.time()
  Sys.sleep(x)
  proc.time() - p1
}

#specify tokens and authorize
#Note: replace asterisk with real keys

consumer_key <- '*******************************' 
consumer_secret <- '*******************************'
access_token <- '*******************************'
access_secret <- '*******************************'

setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)

token <- create_token(
  app = "AppName", #App name
  consumer_key = consumer_key,
  consumer_secret = consumer_secret)

```


### Start download


```{r, message=FALSE, eval=FALSE}

#Define the keywords for subject A
keywords <- c("police", "policing", "law enforcement")

#tweets holder
all_Tweets <- NULL

#Loop through each keyword and wait for 15 minutes 
#and row-bind the results 
for(i in seq_len(length(keywords))){
  
  tweets_g1 <- NULL

  tweets_g1 <- search_tweets(q=keywords[i],  n=17500, type="recent", include_rts=TRUE, 
                             token = token, lang="en",geocode='53.805,-4.242,350mi')
  
  if(nrow(tweets_g1)!=0){
    tweets_g1 <- tweets_g1 %>% dplyr::mutate(class=keywords[i])
    all_Tweets <- rbind(all_Tweets, tweets_g1)
  }
  
  flush.console()
  print(paste(nrow(tweets_g1), nrow(tweets_g1), sep="||"))
  print("waiting for 15.5 minutes")
  waitFun(960)
}

#save the output
write_as_csv(all_Tweets, "tweets.csv", na="NA", fileEncoding = "UTF-8")

```




```{r, eval=FALSE, echo=FALSE, include=FALSE}

#clustering
akObj <- akclustr(prop_crime_per200_people, id_field = TRUE, 
                                method = "linear", k = c(3,8), crit = "Calinski_Harabasz", verbose=TRUE)
```

In order to preview all the variables of the `quality_plot` object, type: 

```{r, echo=TRUE, message=FALSE, eval=FALSE}

names(akObj)

```

* The description of these variables are as follow:

  + `traj` - returns the input data set used for the clustering.

  + `id_field` - indicates whether the input data set included the id field.

  + `solutions` - the list of cluster solutions by `k` values.

  + `qualitycriterion` - the quality criterion specified.

  + `optimal_k` - the optimal value of `k` as determined by the quality criterion.

  + `qualityCrit.List` - the estimated quality of cluster solutions by `k` values.

  + `qltyplot` - the plot of `qualityCrit.List`, with a red vertical line to indicate the optimal value of `k`.



# Conclusion

The `akmedoids` package has been developed in order to aid the replication of a place-based crime inequality investigation conducted in Adepeju et al. (2019). Meanwhile, the utility of the functions in this package are not limited to criminology, but rather can be applicable to longitudinal datasets more generally. This package is being updated on a regular basis to add more functionalities to the existing `functions` and add new functions to carry out other longitudinal data analysis. 

We encourage users to report any bugs encountered while using the package so that they can be fixed immediately. Welcome contributions to this package which will be acknowledged accordingly. 

# References
